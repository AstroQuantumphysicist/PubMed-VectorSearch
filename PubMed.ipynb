{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FAISS AI-Search für PubMed\n",
        "Mithilfe dieses Scripts kann man Artikel aus der PubMed Datenbank auf Relevanz für eine vorgegebene These mithilfe von MSR BiomedBERT.\n",
        "\n",
        "© Lino Brendler, 2025\n"
      ],
      "metadata": {
        "id": "te83PKLsKyWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pubmed_parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u-cJc59MutQk",
        "outputId": "c700ff32-3d73-4a8c-8f8a-69bcd96e3b9e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pubmed_parser in /usr/local/lib/python3.11/dist-packages (0.5.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pubmed_parser) (5.3.1)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (from pubmed_parser) (1.3.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pubmed_parser) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pubmed_parser) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pubmed_parser) (1.26.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pubmed_parser) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pubmed_parser) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pubmed_parser) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pubmed_parser) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wcpaSQqQu554",
        "outputId": "04e85109-7fa9-4dce-e7f0-cd824cf79063"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.43)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.20)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.13)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain-community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain-community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_t4d5WLkAk0J",
        "outputId": "a55b368e-6392-4164-c965-691df6aaf215"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "o11z4qXetXAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f3e057-de46-4c9b-a9a9-58bb5381c5f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Suche nach Artikeln zum Thema '(machine learning[Title/Abstract]) AND (cancer[Title/Abstract])'...\n",
            "📄 229 Artikel gefunden. Erstelle FAISS-Datenbank...\n",
            "🔍 Suche nach relevanten Publikationen zur These...\n",
            "Title: Continuing Medical Education Questions: August 2024.\n",
            "Authors: Shifa Umar\n",
            "PMID: 39724582\n",
            "Abstract: Article Title: Machine Learning Models for Pancreatic Cancer Risk Prediction Using Electronic Health Record Data-A Systematic Review and Assessment.\n",
            "\n",
            "\n",
            "Title: An Adaptive Dendritic Neural Model for Lung Cancer Prediction.\n",
            "Authors: Umair Arif, Chunxia Zhang, Muhammad Waqas Chaudhary, Sajid Hussain\n",
            "PMID: 40026264\n",
            "Abstract: Lung cancer is a leading cause of cancer-related deaths, often diagnosed late due to its aggressive nature. This study presents a novel Adaptive Dendritic Neural Model (ADNM) to enhance diagnostic accuracy in high-dimensional healthcare data. Utilizing hyperparameter optimization and activation mechanisms, ADNM improves scalability and feature selection for multi-class lung cancer prediction. Using a Kaggle dataset, Particle Swarm Optimization (PSO) selected features, while bootstrap assessed performance. ADNM achieved 98.39% accuracy, 99% AUC, and a Cohen's kappa of 96.95%, with rapid convergence via the Adam optimizer, demonstrating its potential for improving early diagnosis and personalized treatment in oncology.\n",
            "\n",
            "\n",
            "Title: Optimizing colorectal polyp detection and localization: Impact of RGB color adjustment on CNN performance.\n",
            "Authors: Jirakorn Jamrasnarodom, Pharuj Rajborirug, Pises Pisespongsa, Kitsuchart Pasupa\n",
            "PMID: 39975856\n",
            "Abstract: Colorectal cancer, arising from adenomatous polyps, is a leading cause of cancer-related mortality, making early detection and removal crucial for preventing cancer progression. Machine learning is increasingly used to enhance polyp detection during colonoscopy, the gold standard for colorectal cancer screening, despite its operator-dependent miss rates. This study explores the impact of RGB color adjustment on Convolutional Neural Network (CNN) models for improving polyp detection and localization in colonoscopic images. Using datasets from Harvard Dataverse for training and internal validation, and LDPolypVideo-Benchmark for external validation, RGB color adjustments were applied, and YOLOv8s was used to develop models. Bayesian optimization identified the best RGB adjustments, with performance assessed using mean average precision (mAP) and F\n",
            "\n",
            "\n",
            "Title: Efficient Normalized Conformal Prediction and Uncertainty Quantification for Anti-Cancer Drug Sensitivity Prediction with Deep Regression Forests.\n",
            "Authors: Daniel Nolte, Souparno Ghosh, Ranadip Pal\n",
            "PMID: 40039751\n",
            "Abstract: Deep learning models are being adopted and applied across various critical medical tasks, yet they are primarily trained to provide point predictions without providing degrees of confidence. Medical practitioner's trustworthiness of deep learning models is increased when paired with uncertainty estimations. Conformal Prediction has emerged as a promising method to pair machine learning models with prediction intervals, allowing for a view of the model's uncertainty. However, popular uncertainty estimation methods for conformal prediction fail to provide highly accurate heteroskedastic intervals. In this paper, we propose a method to estimate the uncertainty of each sample by calculating the variance obtained from a Deep Regression Forest. We show that the deep regression forest variance improves the efficiency and coverage of normalized inductive conformal prediction when applied on an anti-cancer drug sensitivity prediction task.\n",
            "\n",
            "\n",
            "Title: Predicting the anticancer activity of indole derivatives: A novel GP-tree-based QSAR model optimized by ALO with insights from molecular docking and decision-making methods.\n",
            "Authors: Mohamed Kouider Amar, Hamza Moussa, Mohamed Hentabli\n",
            "PMID: 40058079\n",
            "Abstract: Indole derivatives have demonstrated significant potential as anticancer agents; however, the complexity of their structure-activity relationships and the high dimensionality of molecular descriptors present challenges in the drug discovery process. This study addresses these challenges by introducing a modified GP-Tree feature selection algorithm specifically designed for regression tasks and high-dimensional feature spaces. The algorithm effectively identifies relevant descriptors for predicting LogIC\n",
            "\n",
            "\n",
            "Title: Refining breast cancer classification: Customized attention integration approaches with dense and residual networks for enhanced detection.\n",
            "Authors: Mohammad Sakif Alam, Anwar Hossain Efat, Sm Mahedy Hasan, Md Palash Uddin\n",
            "PMID: 39777055\n",
            "Abstract: Breast cancer detection is critical for timely and effective treatment, and automatic detection systems can significantly reduce human error and improve diagnosis speed. This study aims to develop an accurate and robust framework for classifying breast cancer into benign and malignant categories using a novel machine learning architecture. We propose a dense-ResNet attention integration (DRAI) architecture that combines DenseNet and ResNet models with three attention mechanisms to enhance feature extraction from the BreakHis dataset. The attention mechanisms focus on regionally important features, improving classification accuracy. A triple-level ensemble (TLE) method combines the performance of multiple models, further enhancing prediction accuracy. The proposed DRAI architecture with TLE achieves an accuracy of 99.58% in classifying breast cancer into benign and malignant categories, surpassing existing methodologies. This high accuracy demonstrates the effectiveness of the fusion architecture and its ability to reduce manual errors in breast cancer diagnosis. The DRAI architecture with TLE provides a robust, automated framework for breast cancer classification. Its exceptional accuracy lays a solid foundation for future advancements in automated diagnostics and offers a reliable method for aiding early breast cancer detection.\n",
            "\n",
            "\n",
            "Title: Current and Future Applications of PET Radiomics in Radiation Oncology.\n",
            "Authors: Yong Fan, Steven Joel Feigenberg, Charles B Simone\n",
            "PMID: 39915189\n",
            "Abstract: This review delves into the principles of PET imaging and radiomics, emphasizing their importance in detecting, staging, and monitoring various cancers. It highlights the clinical applications of PET radiomics in oncology, showcasing its impact on personalized cancer care. Additionally, the review addresses challenges such as standardizing PET radiomics, integrating multiomics data, and ethical concerns in clinical decision-making. Future directions are also discussed, including broader applications of PET radiomics in clinical trials, artificial intelligence integration for automated analysis, and incorporating multiomics data for a comprehensive understanding of tumor biology.\n",
            "\n",
            "\n",
            "Title: Advancing optical nanosensors with artificial intelligence: A powerful tool to identify disease-specific biomarkers in multi-omics profiling.\n",
            "Authors: Bakr Ahmed Taha, Zahraa Mustafa Abdulrahm, Ali J Addie, Adawiya J Haider, Ali Najem Alkawaz, Isam Ahmed M Yaqoob, Norhana Arsad\n",
            "PMID: 39919475\n",
            "Abstract: Multi-omics profiling integrates genomic, epigenomic, transcriptomic, and proteomic data, essential for understanding complex health and disease pathways. This review highlights the transformative potential of combining optical nanosensors with artificial intelligence (AI). It is possible to identify disease-specific biomarkers using real-time and sensitive molecular interactions. These technologies are precious for genetic, epigenetic, and proteomic changes critical to disease progression and treatment response. AI improves multi-omics profiling by analyzing large, diverse data sets and common patterns traditional methods overlook. Machine learning tools Biomarkers Discovery is revolutionizing, drug resistance is being understood, and medicine is being personalized as the combination of AI and nanosensors has advanced the detection of DNA methylation and proteomic signatures and improved our understanding of cancer, cardiovascular disease and vascular disease. Despite these advances, challenges still exist. Difficulties in integrating data sets, retaining sensors, and building scalable computing tools are the biggest obstacles. It also examines various solutions with advanced AI algorithms and innovations, including fabrication in nanosensor design. Moreover, it highlights the potential of nanosensor-assisted, AI-driven multi-omics profiling to revolutionize disease diagnosis and treatment. As technology advances, these tools pave the way for faster diagnosis, more accurate treatment and improved patient outcomes, offering new hope for personalized medicine.\n",
            "\n",
            "\n",
            "Title: On F\n",
            "Authors: Marwan Alsharman, Hani Samawi, Jing Kersey, Divine Wanduku\n",
            "PMID: 40017014\n",
            "Abstract: Accurate differentiation between health states - diseased or non-diseased - is essential in clinical diagnostics. Optimal cut-off points, or thresholds used to classify test results, are crucial for precise diagnoses. This work introduces the Harmonic Mean of F-score and inverse F-score (\n",
            "\n",
            "\n",
            "Title: Accelerating biopharmaceutical cell line selection with label-free multimodal nonlinear optical microscopy and machine learning.\n",
            "Authors: Jindou Shi, Alexander Ho, Corey E Snyder, Eric J Chaney, Janet E Sorrells, Aneesh Alex, Remben Talaban, Darold R Spillman, Marina Marjanovic, Minh Doan, Gary Finka, Steve R Hood, Stephen A Boppart\n",
            "PMID: 39900674\n",
            "Abstract: The selection of high-performing cell lines is crucial for biopharmaceutical production but is often time-consuming and labor-intensive. We investigated label-free multimodal nonlinear optical microscopy for non-perturbative profiling of biopharmaceutical cell lines based on their intrinsic molecular contrast. Employing simultaneous label-free autofluorescence multiharmonic (SLAM) microscopy with fluorescence lifetime imaging microscopy (FLIM), we characterized Chinese hamster ovary (CHO) cell lines at early passages (0-2). A machine learning (ML)-assisted analysis pipeline leveraged high-dimensional information to classify single cells into their respective lines. Remarkably, the monoclonal cell line classifiers achieved balanced accuracies exceeding 96.8% as early as passage 2. Correlation features and FLIM modality played pivotal roles in early classification. This integrated optical bioimaging and machine learning approach presents a promising solution to expedite cell line selection process while ensuring identification of high-performing biopharmaceutical cell lines. The techniques have potential for broader single-cell characterization applications in stem cell research, immunology, cancer biology and beyond.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import requests\n",
        "\n",
        "# Import\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "########################################\n",
        "# 1️⃣ BERT-basiertes Modell für Embeddings\n",
        "########################################\n",
        "\n",
        "# Modell: MSR BiomedBERT (speziell für biomed. Texte)\n",
        "# Quelle: https://huggingface.co/microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\n",
        "embedding_model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "\n",
        "# Wir erzeugen ein Embedding-Objekt.\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name\n",
        ")\n",
        "\n",
        "########################################\n",
        "# 2️⃣ XML-Parser: Titel, Abstract, Autoren\n",
        "########################################\n",
        "def parse_minimal_pubmed_xml(xml_str):\n",
        "    try:\n",
        "        root = ET.fromstring(xml_str)\n",
        "    except ET.ParseError:\n",
        "        return {\n",
        "            'title': '',\n",
        "            'abstract': '',\n",
        "            'authors': []\n",
        "        }\n",
        "\n",
        "    article_el = root.find('.//Article')\n",
        "    if article_el is None:\n",
        "        return {\n",
        "            'title': '',\n",
        "            'abstract': '',\n",
        "            'authors': []\n",
        "        }\n",
        "\n",
        "    title = article_el.findtext('ArticleTitle', default='')\n",
        "\n",
        "    abstract_texts = article_el.findall('.//AbstractText')\n",
        "    full_abstract = ' '.join(\n",
        "        t.text for t in abstract_texts if t is not None and t.text\n",
        "    )\n",
        "\n",
        "    authors_el = article_el.find('.//AuthorList')\n",
        "    authors = []\n",
        "    if authors_el is not None:\n",
        "        for author in authors_el.findall('Author'):\n",
        "            last_name = author.findtext('LastName', '').strip()\n",
        "            fore_name = author.findtext('ForeName', '').strip()\n",
        "            name = (fore_name + ' ' + last_name).strip()\n",
        "            if name:\n",
        "                authors.append(name)\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'abstract': full_abstract,\n",
        "        'authors': authors\n",
        "    }\n",
        "\n",
        "########################################\n",
        "# 3️⃣ PubMed-Artikel abrufen\n",
        "########################################\n",
        "def get_pubmed_articles(topic, max_results=100):\n",
        "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "    params = {\n",
        "        \"db\": \"pubmed\",\n",
        "        \"term\": topic,  # Nur nach thematisch relevanten Artikeln suchen\n",
        "        \"retmode\": \"json\",\n",
        "        \"retmax\": max_results\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    ids = response.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
        "\n",
        "    articles = []\n",
        "    for pmid in ids:\n",
        "        article_data = get_pubmed_abstract(pmid)\n",
        "        if article_data and article_data.get('abstract'):\n",
        "            articles.append(article_data)\n",
        "    return articles\n",
        "\n",
        "\n",
        "def get_pubmed_abstract(pmid):\n",
        "    fetch_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={pmid}&retmode=xml\"\n",
        "    response = requests.get(fetch_url)\n",
        "    if response.status_code != 200:\n",
        "        return None\n",
        "\n",
        "    parsed = parse_minimal_pubmed_xml(response.text)\n",
        "    if not parsed.get('title') and not parsed.get('abstract'):\n",
        "        return None\n",
        "\n",
        "    authors_str = \", \".join(parsed['authors'])\n",
        "\n",
        "    return {\n",
        "        \"pmid\": pmid,\n",
        "        \"title\": parsed['title'],\n",
        "        \"abstract\": parsed['abstract'],\n",
        "        \"authors\": authors_str\n",
        "    }\n",
        "\n",
        "########################################\n",
        "# 4️⃣ Erstelle FAISS-Vektordatenbank\n",
        "########################################\n",
        "def build_faiss_db(articles):\n",
        "    texts = [a[\"abstract\"] for a in articles]\n",
        "    metadata = [\n",
        "        {\n",
        "            \"pmid\": a[\"pmid\"],\n",
        "            \"title\": a[\"title\"],\n",
        "            \"authors\": a.get(\"authors\", \"\")\n",
        "        }\n",
        "        for a in articles\n",
        "    ]\n",
        "    faiss_db = FAISS.from_texts(texts, embedding_model, metadatas=metadata)\n",
        "    return faiss_db\n",
        "\n",
        "########################################\n",
        "# 5️⃣ Semantische Suche in der FAISS-Datenbank\n",
        "########################################\n",
        "def search_pubmed(thesis, db, k=10):\n",
        "    similar = db.similarity_search(thesis, k=k)\n",
        "    results = []\n",
        "    for doc in similar:\n",
        "        results.append(\n",
        "            f\"Title: {doc.metadata['title']}\\n\" +\n",
        "            f\"Authors: {doc.metadata['authors']}\\n\" +\n",
        "            f\"PMID: {doc.metadata['pmid']}\\n\" +\n",
        "            f\"Abstract: {doc.page_content}\\n\"\n",
        "        )\n",
        "    return \"\\n\\n\".join(results)\n",
        "\n",
        "########################################\n",
        "# 7️⃣ Aufruf\n",
        "########################################\n",
        "topic = \"(machine learning[Title/Abstract]) AND (cancer[Title/Abstract])\" # hier PubMed Search Term eingeben\n",
        "thesis = \"Machine learning improves cancer diagnosis\" # hier These eingeben\n",
        "\n",
        "print(f\"📄 Suche nach Artikeln zum Thema '{topic}'...\") # sucht nach Artikeln zu dem Search Term\n",
        "articles = get_pubmed_articles(topic, max_results=1000) # hier Maximale Ergebnis Zahl einsetzen, für genaue Analyse ab 10_000 (dauert dann aber entsprechend länger)\n",
        "print(f\"📄 {len(articles)} Artikel gefunden. Erstelle FAISS-Datenbank...\") # erstellung der FAISS Vector-Search Datenbank\n",
        "faiss_db = build_faiss_db(articles)\n",
        "\n",
        "print(\"🔍 Suche nach relevanten Publikationen zur These...\") # heraussuchen der relevanten artikel mithilfe von FAISS\n",
        "search_results = search_pubmed(thesis, faiss_db, k=10) # k-Wert anpassen, so höher desto ungenauer die Auswahl, desto mehr Artikel bleiben übrig, kleiner k-Wert bedeutet genauere Ergebnisse, dafür weniger\n",
        "print(search_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zuerst folgende Vorlage kopieren (nach ChatGPT):\n",
        "\n",
        "```\n",
        "Hier ist eine These:\n",
        "[These]\n",
        "Ich habe mehrere Artikel herausgesucht, mit deren Hilfe möchte ich die These stärken. Nenne für jeden artikel ob er relevant ist oder nicht in folgendem Format:\n",
        "\n",
        "---\n",
        "Titel: [Titel]\n",
        "PMID: [PMID]\n",
        "Relevanz: [Relevanz von 1 (komplett irrelevant) bis 10 (100% relevant)]\n",
        "Begründung der Relevanz: [Begründung]\n",
        "---\n",
        "[Nächste Artikel im selben Format verarbeiten!]\n",
        "---\n",
        "\n",
        "Hier sind die Artikel:\n",
        "[Artikel]\n",
        "\n",
        "```\n",
        "\n",
        "Schreiben sie in `[These]` ihre These mit der sie gesucht haben (also von oben kopieren). In `[Artikel]` müssen sie alle Artikel im Format von oben reinkopieren.\n",
        "Jetzt müssen sie sich nur noch die Ergebnisse von ChatGPT durchschauen."
      ],
      "metadata": {
        "id": "xHsRYx13NOfy"
      }
    }
  ]
}